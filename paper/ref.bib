@book{VanDalen1994,
address = {Berlin, Heidelberg},
annote = {The process of formalization of propositional logic consists of two stages:
(1) present a formal language, (2) specify a procedure for obtaining valid or
true propositions.},
author = {van Dalen, Dirk},
doi = {10.1007/978-3-662-02962-6},
file = {:Users/jonaprieto/Mendeley/van Dalen - 1994 - Logic and Structure.pdf:pdf},
isbn = {978-3-540-57839-0},
publisher = {Springer Berlin Heidelberg},
series = {Universitext},
title = {{Logic and Structure}},
url = {http://link.springer.com/10.1007/978-3-662-02962-6},
year = {1994}
}
@article{Stefanowski2001,
abstract = {The rough set theory, based on the original definition of the indiscernibility relation, is not useful for analysing incomplete information tables where some values of attributes are unknown. In this paper we distinguish two different semantics for incomplete information: the "missing value" semantics and the "absent value" semantics. The already known approaches, e.g. based on the tolerance relations, deal with the missing value case. We introduce two generalisations of the rough sets theory to handle these situations. The first generalisation introduces the use of a non symmetric similarity relation in order to formalise the idea of absent value semantics. The second proposal is based on the use of valued tolerance relations. A logical analysis and the computational experiments show that for the valued tolerance approach it is possible to obtain more informative approximations and decision rules than using the approach based on the simple tolerance relation.},
author = {Stefanowski, J. and Tsouki{\`{a}}s, A.},
doi = {10.1111/0824-7935.00162},
file = {:Users/jonaprieto/Mendeley/Stefanowski, Tsouki{\`{a}}s - 2001 - Incomplete information tables and rough classification.pdf:pdf},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {Decision rules,Fuzzy sets,Incomplete information,Rough sets,Similarity relation,Valued tolerance relation},
number = {3},
pages = {545--566},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@article{Agudelo-Agudelo2017,
author = {Agudelo-Agudelo, Juan C.},
doi = {10.1007/s11787-017-0168-1},
file = {:Users/jonaprieto/Mendeley/Agudelo-Agudelo - 2017 - Translating Non-classical Logics into Classical Logic by Using Hidden Variables.pdf:pdf},
issn = {1661-8297},
journal = {Logica Universalis},
number = {2},
pages = {205--224},
title = {{Translating Non-classical Logics into Classical Logic by Using Hidden Variables}},
url = {http://link.springer.com/10.1007/s11787-017-0168-1},
volume = {11},
year = {2017}
}
@article{Bove2002,
abstract = {General recursive algorithms are such that the recursive calls are performed on arguments satisfying no condition that guarantees termination. Hence, there is no direct way of formalising them in type theory. The standard way of handling general recursion in type theory uses a well-founded recursion principle. Unfortunately, this way of formalising general recursive algorithms often produces unnecessarily long and complicated codes. On the other hand, functional programming languages like Haskell impose no restrictions on recursive programs, and then writing general recursive algorithms is straightforward. In addition, functional programs are usually short and self-explanatory. However, the existing frameworks for reasoning about the correctness of Haskell-like programs are weaker than the framework provided by type theory. The goal of this work is to present a method that combines the advantages of both programming styles when writing simple general recursive algorithms....},
author = {Bove, Ana},
doi = {10.1017/S0960129505004822},
file = {:Users/jonaprieto/Mendeley/Bove - 2002 - General recursion in type theory.pdf:pdf},
issn = {0346718X},
journal = {Doktorsavhandlingar vid Chalmers Tekniska Hogskola},
number = {1889},
pages = {39--58},
title = {{General recursion in type theory}},
year = {2002}
}
@incollection{Bove2005,
author = {Bove, Ana and Capretta, Venanzio},
doi = {10.1007/11417170_10},
file = {:Users/jonaprieto/Mendeley/Bove, Capretta - 2005 - Recursive Functions with Higher Order Domains.pdf:pdf},
pages = {116--130},
publisher = {Springer, Berlin, Heidelberg},
title = {{Recursive Functions with Higher Order Domains}},
url = {http://link.springer.com/10.1007/11417170_10},
year = {2005}
}
@article{stefanowski2001incomplete,
author = {Stefanowski, Jerzy and Tsoukias, Alexis},
journal = {Computational Intelligence},
number = {3},
pages = {545--566},
publisher = {Wiley Online Library},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@inproceedings{nauck1999learning,
author = {Nauck, Detlef and Kruse, Rudolf},
booktitle = {Neural Information Processing, 1999. Proceedings. ICONIP'99. 6th International Conference on},
organization = {IEEE},
pages = {142--147},
title = {{Learning in neuro-fuzzy systems with symbolic attributes and missing values}},
volume = {1},
year = {1999}
}
@article{wang2005classification,
author = {Wang, Shouhong},
journal = {Computers {\&} operations research},
number = {10},
pages = {2583--2594},
publisher = {Elsevier},
title = {{Classification with incomplete survey data: a Hopfield neural network approach}},
volume = {32},
year = {2005}
}
@article{slowinski1989rough,
author = {S{\l}owi{\'{n}}ski, Roman and Stefanowski, Jerzy},
journal = {Mathematical and Computer Modelling},
number = {10},
pages = {1347--1357},
publisher = {Elsevier},
title = {{Rough classification in incomplete information systems}},
volume = {12},
year = {1989}
}
@article{chiu1986synthesizing,
author = {Chiu, David K Y and Wong, Andrew K C},
journal = {Systems, Man and Cybernetics, IEEE Transactions on},
number = {2},
pages = {251--259},
publisher = {IEEE},
title = {{Synthesizing knowledge: A cluster analysis approach using event covering}},
volume = {16},
year = {1986}
}
@inbook{Gantayat2014,
address = {Cham},
author = {Gantayat, S S and Misra, Ashok and Panda, B S},
booktitle = {Proceedings of the International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2013},
doi = {10.1007/978-3-319-02931-3_45},
editor = {Satapathy, Suresh Chandra and Udgata, Siba K and Biswal, Bhabendra Narayan},
isbn = {978-3-319-02931-3},
pages = {401--408},
publisher = {Springer International Publishing},
title = {{A Study of Incomplete Data -- A Review}},
url = {http://dx.doi.org/10.1007/978-3-319-02931-3_45},
year = {2014}
}
@article{wong1987synthesizing,
author = {Wong, Andrew K C and Chiu, David K Y},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {6},
pages = {796--805},
publisher = {IEEE},
title = {{Synthesizing statistical knowledge from incomplete mixed-mode data}},
year = {1987}
}
@article{schafer2002missing,
author = {Schafer, Joseph L and Graham, John W},
journal = {Psychological methods},
number = {2},
pages = {147},
publisher = {American Psychological Association},
title = {{Missing data: our view of the state of the art.}},
volume = {7},
year = {2002}
}
@article{nelwamondo2007missing,
author = {Nelwamondo, Fulufhelo V and Mohamed, Shakir and Marwala, Tshilidzi},
journal = {arXiv preprint arXiv:0704.3474},
title = {{Missing data: A comparison of neural network and expectation maximisation techniques}},
year = {2007}
}
@article{pawlak1998rough,
author = {Pawlak, Zdzislaw},
journal = {Cybernetics {\&} Systems},
number = {7},
pages = {661--688},
publisher = {Taylor {\&} Francis},
title = {{Rough set theory and its applications to data analysis}},
volume = {29},
year = {1998}
}
@misc{keeler1998method,
annote = {US Patent 5,842,189},
author = {Keeler, James David and Hartman, Eric Jon and Ferguson, Ralph Bruce},
publisher = {Google Patents},
title = {{Method for operating a neural network with missing and/or incomplete data}},
year = {1998}
}
@article{saar2007handling,
author = {Saar-Tsechansky, Maytal and Provost, Foster},
journal = {Journal of machine learning research},
number = {Jul},
pages = {1623--1657},
title = {{Handling missing values when applying classification models}},
volume = {8},
year = {2007}
}
@inproceedings{mohamed2005neural,
author = {Mohamed, Shakir and Marwala, Tshilidzi},
booktitle = {16th Annual Symposium of the Patten Recognition Association of South Africa, Langebaan},
pages = {27--32},
title = {{Neural network based techniques for estimating missing data in databases}},
year = {2005}
}
@incollection{grzymala2006rough2,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Rough Sets and Knowledge Technology},
pages = {58--67},
publisher = {Springer},
title = {{A rough set approach to data with missing attribute values}},
year = {2006}
}
@article{jerez2010missing,
author = {Jerez, Jos{\'{e}} M and Molina, Ignacio and Garc$\backslash$'$\backslash$ia-Laencina, Pedro J and Alba, Emilio and Ribelles, Nuria and Mart$\backslash$'$\backslash$in, Miguel and Franco, Leonardo},
journal = {Artificial intelligence in medicine},
number = {2},
pages = {105--115},
publisher = {Elsevier},
title = {{Missing data imputation using statistical and machine learning methods in a real breast cancer problem}},
volume = {50},
year = {2010}
}
@incollection{grzymala2006rough,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Foundations and novel approaches in data mining},
pages = {197--212},
publisher = {Springer},
title = {{Rough set strategies to data with missing attribute values}},
year = {2006}
}
@incollection{stefanowski1999extension,
author = {Stefanowski, Jerzy and Tsouki{\`{a}}s, Alexis},
booktitle = {New Directions in Rough Sets, Data Mining, and Granular-Soft Computing},
pages = {73--81},
publisher = {Springer},
title = {{On the extension of rough sets under incomplete information}},
year = {1999}
}
@article{garciarena2016investigation,
author = {{Garciarena Hualde}, Unai},
title = {{An investigation of imputation methods for discrete databases and multi-variate time series}},
year = {2016}
}
@article{pawlak1982rough,
author = {Pawlak, Zdzis{\l}aw},
journal = {International Journal of Computer {\&} Information Sciences},
number = {5},
pages = {341--356},
publisher = {Springer},
title = {{Rough sets}},
volume = {11},
year = {1982}
}
@incollection{grzymala2008three,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Data Mining: Foundations and Practice},
pages = {139--152},
publisher = {Springer},
title = {{Three approaches to missing attribute values: A rough set perspective}},
year = {2008}
}
@article{gediga2003maximum,
author = {Gediga, G{\"{u}}nther and D{\"{u}}ntsch, Ivo},
journal = {Artificial Intelligence Review},
number = {1},
pages = {93--107},
publisher = {Springer},
title = {{Maximum consistency of incomplete data via non-invasive imputation}},
volume = {19},
year = {2003}
}
@article{hong2002learning,
author = {Hong, Tzung-Pei and Tseng, Li-Huei and Wang, Shyue-Liang},
journal = {Expert Systems with Applications},
number = {4},
pages = {285--293},
publisher = {Elsevier},
title = {{Learning rules from incomplete training examples by rough sets}},
volume = {22},
year = {2002}
}
@inproceedings{kerber1992chimerge,
author = {Kerber, Randy},
booktitle = {Proceedings of the tenth national conference on Artificial intelligence},
organization = {Aaai Press},
pages = {123--128},
title = {{Chimerge: Discretization of numeric attributes}},
year = {1992}
}
@incollection{Pawlak2004,
author = {Pawlak, Zdzis{\l}aw},
booktitle = {Transactions on Rough Sets I},
pages = {1--58},
publisher = {Springer},
title = {{Some issues on rough sets}},
year = {2004}
}
@inproceedings{Quinlan1989,
author = {Quinlan, John Ross},
booktitle = {Proc. of the Sixth Int. Workshop on Machine Learning},
pages = {164--168},
title = {{Unknown attribute values in induction}},
year = {1989}
}
@inproceedings{abdella2005use,
author = {Abdella, Mussa and Marwala, Tshilidzi},
booktitle = {Computational Cybernetics, 2005. ICCC 2005. IEEE 3rd International Conference on},
organization = {IEEE},
pages = {207--212},
title = {{The use of genetic algorithms and neural networks to approximate missing data in database}},
year = {2005}
}
@article{kryszkiewicz1998rough,
author = {Kryszkiewicz, Marzena},
journal = {Information sciences},
number = {1},
pages = {39--49},
publisher = {Elsevier},
title = {{Rough set approach to incomplete information systems}},
volume = {112},
year = {1998}
}
@inproceedings{lakshminarayan1996imputation,
author = {Lakshminarayan, Kamakshi and Harp, Steven A and Goldman, Robert P and Samad, Tariq and Others},
booktitle = {KDD},
pages = {140--145},
title = {{Imputation of Missing Data Using Machine Learning Techniques.}},
year = {1996}
}
@inproceedings{dougherty1995supervised,
author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran and Others},
booktitle = {Machine learning: proceedings of the twelfth international conference},
pages = {194--202},
title = {{Supervised and unsupervised discretization of continuous features}},
volume = {12},
year = {1995}
}
@inbook{Li2005,
address = {Berlin, Heidelberg},
author = {Li, Dan and Deogun, Jitender and Spaulding, William and Shuart, Bill},
booktitle = {Transactions on Rough Sets IV},
doi = {10.1007/11574798_3},
editor = {Peters, James F and Skowron, Andrzej},
isbn = {978-3-540-32016-6},
pages = {37--57},
publisher = {Springer Berlin Heidelberg},
title = {{Dealing with Missing Data: Algorithms Based on Fuzzy Set and Rough Set Theories}},
url = {http://dx.doi.org/10.1007/11574798_3},
year = {2005}
}
@article{junninen2004methods,
author = {Junninen, Heikki and Niska, Harri and Tuppurainen, Kari and Ruuskanen, Juhani and Kolehmainen, Mikko},
journal = {Atmospheric Environment},
number = {18},
pages = {2895--2907},
publisher = {Elsevier},
title = {{Methods for imputation of missing values in air quality data sets}},
volume = {38},
year = {2004}
}
@book{little2014statistical,
author = {Little, Roderick J A and Rubin, Donald B},
publisher = {John Wiley {\&} Sons},
title = {{Statistical analysis with missing data}},
year = {2014}
}
@article{liu2013comparison,
author = {Liu, Yushan and Brown, Steven D},
journal = {Chemometrics and Intelligent Laboratory Systems},
pages = {106--115},
publisher = {Elsevier},
title = {{Comparison of five iterative imputation methods for multivariate classification}},
volume = {120},
year = {2013}
}
@incollection{grzymala2004data,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Transactions on Rough Sets I},
pages = {78--95},
publisher = {Springer},
title = {{Data with missing attribute values: Generalization of indiscernibility relation and rule induction}},
year = {2004}
}
@book{pawlak2012rough,
author = {Pawlak, Zdzis{\l}aw},
publisher = {Springer Science {\&} Business Media},
title = {{Rough sets: Theoretical aspects of reasoning about data}},
volume = {9},
year = {2012}
}
@article{Luo2014,
author = {Luo, Chuan and Li, Tianrui},
doi = {10.1007/978-3-319-08644-6_13},
file = {:Users/jonaprieto/Mendeley/Luo, Li - 2014 - Incremental three-way decisions with incomplete information.pdf:pdf},
isbn = {9783319086439},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {incomplete decision system,incremental updating,three-way decisions},
pages = {128--135},
title = {{Incremental three-way decisions with incomplete information}},
volume = {8536 LNAI},
year = {2014}
}
@incollection{acuna2004treatment,
author = {Acuna, Edgar and Rodriguez, Caroline},
booktitle = {Classification, clustering, and data mining applications},
pages = {639--647},
publisher = {Springer},
title = {{The treatment of missing values and its effect on classifier accuracy}},
year = {2004}
}
@article{Wu2009,
author = {Wu, Xindong and Kumar, Vipin and Quinlan, J Ross and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J and Ng, Angus and Liu, Bing and Philip, S Yu and Others},
journal = {Knowledge and information systems},
number = {1},
pages = {1--37},
publisher = {Springer},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2009}
}
@article{Liu2015,
abstract = {As a natural extension of three-way decisions with incomplete information, this paper provides a novel three-way decision model based on incomplete information system. First, we define a new relation to describe the similarity degree of incomplete information. Then, in view of the missing values presented in incomplete information system, we utilize interval number to acquire the loss function. A hybrid information table which consist both of the incomplete information and loss function, is used to deal with the new three-way decision model. The key steps and algorithm for constructing the integrated three-way decision model are also carefully investigated. An empirical study of medical diagnosis validates the reasonability and effectiveness of our proposed model.},
author = {Liu, Dun and Liang, Decui and Wang, Changchun},
doi = {10.1016/j.knosys.2015.07.036},
file = {:Users/jonaprieto/Mendeley/Liu, Liang, Wang - 2015 - A novel three-way decision model based on incomplete information system.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Decision-theoretic rough sets,Hybrid information system,Incomplete information system,Loss function,Three-way decisions},
number = {July},
pages = {32--45},
publisher = {Elsevier B.V.},
title = {{A novel three-way decision model based on incomplete information system}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115003007},
volume = {91},
year = {2015}
}
@incollection{greco2000dealing,
author = {Greco, SMBSR and Matarazzo, B and Slowinski, R},
booktitle = {Decision making: Recent developments and worldwide applications},
pages = {295--316},
publisher = {Springer},
title = {{Dealing with missing data in rough set analysis of multi-attribute and multi-criteria decision problems}},
year = {2000}
}
@inproceedings{grzymala2000comparison,
author = {Grzymala-Busse, Jerzy W and Hu, Ming},
booktitle = {Rough sets and current trends in computing},
organization = {Springer},
pages = {378--385},
title = {{A comparison of several approaches to missing attribute values in data mining}},
year = {2000}
}
@inproceedings{grzymala2004characteristic,
author = {Grzyma{\l}a-Busse, Jerzy W},
booktitle = {Rough Sets and Current Trends in Computing},
organization = {Springer},
pages = {244--253},
title = {{Characteristic relations for incomplete data: A generalization of the indiscernibility relation}},
year = {2004}
}
@book{quinlan2014c4,
author = {Quinlan, J Ross},
publisher = {Elsevier},
title = {{C4. 5: programs for machine learning}},
year = {2014}
}
@inproceedings{grzymala2004rough,
author = {Grzymala-Busse, Jerzy W and Siddhaye, Sachin},
booktitle = {Proceedings of the IPMU},
pages = {923--930},
title = {{Rough set approaches to rule induction from incomplete data}},
volume = {2},
year = {2004}
}
@article{bai2015novel,
author = {Bai, Xiuling and Zhang, Mingchuan and Wu, Qingtao and Zheng, Ruijuan and Zhao, Haixia and Wei, Wangyang},
journal = {International Journal of Database Theory and Application},
number = {6},
pages = {149--164},
title = {{A Novel Data Filling Algorithm for Incomplete Information System Based on Valued Limited Tolerance Relation}},
volume = {8},
year = {2015}
}
@article{wang2009discovering,
author = {Wang, Hai and Wang, Shouhong},
journal = {Expert Systems with Applications},
number = {3},
pages = {6256--6260},
publisher = {Elsevier},
title = {{Discovering patterns of missing data in survey databases: an application of rough sets}},
volume = {36},
year = {2009}
}
@inproceedings{grzymala1997modified,
author = {Grzymala-Busse, Jerzy W and Wang, Arthur Y},
booktitle = {Proc. of the Fifth International Workshop on Rough Sets and Soft Computing (RSSC'97) at the Third Joint Conference on Information Sciences (JCIS'97), Research Triangle Park, NC},
pages = {69--72},
title = {{Modified algorithms LEM1 and LEM2 for rule induction from data with missing attribute values}},
year = {1997}
}
@article{gabrys2002neuro,
author = {Gabrys, Bogdan},
journal = {International Journal of Approximate Reasoning},
number = {3},
pages = {149--179},
publisher = {Elsevier},
title = {{Neuro-fuzzy approach to processing inputs with missing values in pattern recognition problems}},
volume = {30},
year = {2002}
}
@article{grzymala2003data,
author = {Grzymala-Busse, Jerzy W and Ziarko, Wojciech},
journal = {Data Mining: Opportunities and Challenges},
pages = {142--173},
publisher = {Hershey, PA: Idea Group Publ},
title = {{Data mining based on rough sets}},
volume = {2},
year = {2003}
}
@article{nelwamondo2007rough,
author = {Nelwamondo, Fulufhelo Vincent and Marwala, Tshilidzi},
journal = {arXiv preprint arXiv:0704.3635},
title = {{Rough sets computations to impute missing data}},
year = {2007}
}
@article{kim1977treatment,
author = {Kim, Jae-On and Curry, James},
journal = {Sociological Methods {\&} Research},
number = {2},
pages = {215--240},
publisher = {Sage Publications},
title = {{The treatment of missing data in multivariate analysis}},
volume = {6},
year = {1977}
}
@book{Buurlage,
author = {Buurlage, Jan-willem},
file = {:Users/jonaprieto/Mendeley/Buurlage - Unknown - Categories and Haskell An introduction to the mathematics behind modern functional programming.pdf:pdf},
title = {{Categories and Haskell: An introduction to the mathematics behind modern functional programming}}
}
@article{Sutcliffe1996,
author = {Sutcliffe, Geoff},
file = {:Users/jonaprieto/Mendeley/Sutcliffe - 1996 - The Practice of Clausification in Automatic Theorem Proving.pdf:pdf},
keywords = {automatic theorem proving,clauses,computing review categories,resolution},
number = {18},
pages = {57--68},
title = {{The Practice of Clausification in Automatic Theorem Proving}},
year = {1996}
}
@phdthesis{Sicard2015,
abstract = {We propose a new approach to computer-assisted verification of lazy functional programs where functions can be defined by general recursion. We work in first-order theories of functional programs which are obtained by translating Dybjer's programming logic (Dybjer, P. [1985]. Program Veri- fication in a Logical Theory of Constructions. In: Functional Programming Languages and Computer Architecture. Ed. by Jouannaud, J.-P. Vol. 201. Lecture Notes in Computer Science. Springer, pp. 334–349) into a first-order theory, and by extending this programming logic with new (co-)inductive predicates. Rather than building a special purpose system, we formalise our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interact- ive reasoning by representing first-order theories using the propositions-as- types principle. Further support is provided by off-the-shelf automatic the- orem provers for first-order-logic called by a Haskell program that translates our Agda representations of first-order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proofs by induction and co- induction. The examples include functions defined by structural recursion, simple general recursion, nested recursion, higher-order recursion, guarded and unguarded co-recursion. Keywords:},
author = {Sicard-Ram{\'{i}}rez, Andr{\'{e}}s and Bove, Ana and Dybjer, Peter},
file = {:Users/jonaprieto/Mendeley/Sicard-Ram{\'{i}}rez, Bove, Dybjer - 2015 - Reasoning about Functional Programs by Combining Interactive and Automatic Proofs.pdf:pdf},
keywords = {automatic proofs,correctness,first-order theories,functional program,general recursion,interactive proofs,lazy evaluation,total lan- guages,type theory},
school = {Universidad de la Rep{\{}{\'{u}}{\}}blica},
title = {{Reasoning about Functional Programs by Combining Interactive and Automatic Proofs}},
url = {https://www.colibri.udelar.edu.uy/handle/123456789/4715},
year = {2015}
}
@misc{OnlineATPs,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398851},
title = {{OnlineATPs: A command-line tool client for the TPTP World.}},
url = {https://doi.org/10.5281/zenodo.398851},
year = {2017}
}
@misc{AgdaMetis,
author = {Prieto-Cubides, Jonathan and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
doi = {10.5281/zenodo.398862},
title = {{Agda-Metis Library}},
url = {https://doi.org/10.5281/zenodo.398862},
year = {2017}
}
@misc{Athena,
author = {Prieto-Cubides, Jonathan and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
doi = {10.5281/zenodo.437196},
title = {{Proof Reconstruction of Classical Propositional Logic with Athena}},
url = {https://doi.org/10.5281/zenodo.437196},
year = {2017}
}
@misc{AgdaProp,
author = {Prieto-Cubides, Jonathan and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
doi = {10.5281/zenodo.398852},
title = {{Agda-Prop Library for Classical Propositional Logic}},
url = {https://doi.org/10.5281/zenodo.398852},
year = {2017}
}
@misc{Prieto-Cubides2017,
abstract = {Collection of TPTP problems of propositional logic.},
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/ZENODO.817997},
file = {:Users/jonaprieto/Mendeley/Prieto-Cubides - 2017 - Prop-Pack Collection of TPTP problems of propositional logic.pdf:pdf;:Users/jonaprieto/Mendeley/Prieto-Cubides - 2017 - Prop-Pack Collection of TPTP problems of propositional logic(2).pdf:pdf},
month = {jun},
title = {{Prop-Pack: Collection of TPTP problems of propositional logic}},
url = {https://zenodo.org/record/817997#.WYdjIbb5iAw},
year = {2017}
}
@inproceedings{Armand2010,
abstract = {Coq has within its logic a programming language that can be used to replace many deduction steps into a single computation, this is the so-called reflection. In this paper, we present two extensions of the evaluation mechanism that preserve its correctness and make it possible to deal with cpu-intensive tasks such as proof checking of SAT traces.},
author = {Armand, Michael and Gr{\'{e}}goire, Benjamin and Spiwack, Arnaud and Th{\'{e}}ry, Laurent},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-14052-5_8},
isbn = {3642140513},
issn = {03029743},
pages = {83--98},
title = {{Extending Coq with imperative features and its application to SAT verification}},
volume = {6172 LNCS},
year = {2010}
}
@inproceedings{Bove2012,
abstract = {We propose a new approach to the computer-assisted verification of functional programs. We work in first order theories of functional programs which are obtained by extending Aczel's first order theory of combinatory formal arithmetic with positive inductive and coinductive predicates. Rather than building a special purpose system we implement our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interactive reasoning by encoding first order theories using the formulae-as-types principle. Further support is provided by off-the-shelf automatic theorem provers for first order logic which can be called by a program which translates Agda representations of first order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proof by induction and coinduction.},
author = {Bove, Ana and Dybjer, Peter and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-28729-9_7},
file = {:Users/jonaprieto/Mendeley/Bove, Dybjer, Sicard-Ram{\'{i}}rez - 2012 - Combining Interactive and Automatic Reasoning in First Order Theories of Functional Programs.pdf:pdf},
isbn = {9783642287282},
issn = {03029743},
pages = {104--118},
publisher = {Springer},
title = {{Combining Interactive and Automatic Reasoning in First Order Theories of Functional Programs}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-28729-9_7},
volume = {7213 LNCS},
year = {2012}
}
@article{Ge2008,
abstract = {Satisfiability Modulo Theories (SMT) solvers are large and complicated pieces of code. As a result, ensuring their correctness is challenging. In this paper, we discuss a technique for ensuring soundness by producing and checking proofs. We give details of our implementation using CVC3 and HOL Light and provide initial results from our effort to certify the SMT-LIB benchmarks.},
author = {Ge, Yeting and Barrett, Clark},
file = {:Users/jonaprieto/Mendeley/Ge, Barrett - 2008 - Proof Translation and SMT-LIB Benchmark Certification A Preliminary Report.pdf:pdf},
journal = {Proceedings of the 6{\^{}}{\{}th{\}} International Workshop on Satisfiability Modulo Theories (SMT '08)},
number = {0551645},
pages = {1--11},
title = {{Proof Translation and SMT-LIB Benchmark Certification: A Preliminary Report}},
url = {https://cs.nyu.edu/~yeting/prooftrans.pdf},
year = {2008}
}
@article{Church1940,
abstract = {Simple type theory is formulated for use with the generic theorem prover Isabelle. This requires explicit type inference rules. There are function, product, and subset types, which may be empty. Descriptions (the eta-operator) introduce the Axiom of Choice. Higher-order logic is obtained through reflection between formulae and terms of type bool. Recursive types and functions can be formally constructed. Isabelle proof procedures are described. The logic appears suitable for general mathematics as well as computational problems.},
archivePrefix = {arXiv},
arxivId = {cs/9301107},
author = {Church, Alonzo},
doi = {10.2307/2266170},
eprint = {9301107},
file = {:Users/jonaprieto/Mendeley/Church - 1940 - A Formulation of the Simple Theory of Types.pdf:pdf},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {2},
pages = {56--68},
primaryClass = {cs},
title = {{A Formulation of the Simple Theory of Types}},
url = {http://www.jstor.org/stable/2266170},
volume = {5},
year = {1940}
}
@article{Kanso2012,
author = {Kanso, Karim},
file = {:Users/jonaprieto/Mendeley/Kanso - 2012 - Agda as a Platform for the Development of Verified Railway Interlocking Systems.pdf:pdf},
title = {{Agda as a Platform for the Development of Verified Railway Interlocking Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.310.1502},
year = {2012}
}
@misc{kanso:MResThesis:2013,
abstract = {This project studied whether a digital interlocking which had been pro- grammed with ladder logic (Boolean program) would obey generic safety properties. This was carried out by translating the ladder logic into an alternate representation and applying various techniques to allow specifica- tion of safety properties. Finally, a proof engine was used to formally verify if these properties were fulfilled and if they are not, then human readable documentation would be generated. III},
annote = {{\{}M{\}}aster of {\{}R{\}}esearch thesis, Dept.{\~{}}of Computer Science, Swansea University, Swansea SA2 8PP, UK.
Available from http://www.swan.ac.uk/{\$}{\~{}}{\$}csetzer/articlesFromOthers/index.html},
author = {Kanso, Karim},
file = {:Users/jonaprieto/Mendeley/Kanso - 2010 - Formal Verification of Ladder Logic.pdf:pdf},
institution = {Swansea University},
title = {{Formal Verification of Ladder Logic}},
url = {http://www.cs.swan.ac.uk/~csetzer/articlesFromOthers/kanso/karimKansoMResThesisFormalVerificationOfLadderLogic.pdf},
year = {2010}
}
@inproceedings{Stump2008,
address = {New York, New York, USA},
author = {Stump, Aaron and Oe, Duckki},
booktitle = {Proceedings of the Joint Workshops of the 6th International Workshop on Satisfiability Modulo Theories and 1st International Workshop on Bit-Precise Reasoning - SMT '08/BPR '08},
doi = {10.1145/1512464.1512470},
file = {:Users/jonaprieto/Mendeley/Stump, Oe - 2008 - Towards an SMT proof format.pdf:pdf;:Users/jonaprieto/Mendeley/Stump, Oe - 2008 - Towards an SMT proof format(2).pdf:pdf},
isbn = {9781605584409},
keywords = {logical framework,proof formats},
pages = {27},
publisher = {ACM Press},
title = {{Towards an SMT proof format}},
url = {http://portal.acm.org/citation.cfm?doid=1512464.1512470},
year = {2008}
}
@article{Tammet1997,
abstract = {We give a brief overview of the first-order classical logic component in the Gandalf family of resolution-based automated theorem provers for classical and intuitionistic logics. The main strength of the described version is a sophisticated algorithm for nonunit subsumption. {\textcopyright} 1997 Kluwer Academic Publishers.},
author = {Tammet, Tanel},
file = {:Users/jonaprieto/Mendeley/Tammet - 1996 - A Resolution Theorem Prover for Intuitionistic Logic.pdf:pdf},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Automated theorem proving,Competition,Gandalf,Resolution,Subsumption},
number = {2},
pages = {199--204},
title = {{Gandalf}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0031108576&partnerID=tZOtx3y1},
volume = {18},
year = {1997}
}
@inproceedings{Brown2012,
abstract = {Satallax is an automatic higher-order theorem prover that generates propositional clauses encoding (ground) tableau rules and uses MiniSat to test for unsatisfiability. We describe the implementation, focusing on flags that control search and examples that illustrate how the search proceeds.},
author = {Brown, Chad E.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-31365-3_11},
isbn = {9783642313646},
issn = {03029743},
keywords = {higher-order logic,higher-order theorem proving,simple type theory},
pages = {111--117},
title = {{Satallax: An automatic higher-order prover}},
volume = {7364 LNAI},
year = {2012}
}
@inproceedings{Benzmuller2008,
abstract = {LEO-II is a standalone, resolution-based higher-order theorem prover designed for effective cooperation with specialist provers for natural fragments of higher-order logic. At present LEO-II can cooperate with the first-order automated theorem provers E, SPASS, and Vampire. The improved performance of LEO-II, especially in comparison to its predecessor LEO, is due to several novel features including the exploitation of term sharing and term indexing techniques, support for primitive equality reasoning, and improved heuristics at the calculus level. LEO-II is implemented in Objective Caml and its problem representation language is the new TPTP THF language.},
author = {Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C and Theiss, Frank and Fietzke, Arnaud},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-71070-7_14},
isbn = {3540710698},
issn = {03029743},
pages = {162--170},
title = {{LEO-II - A cooperative automatic theorem prover for classical higher-order logic (system description)}},
volume = {5195 LNAI},
year = {2008}
}
@inproceedings{Lonsing2017,
abstract = {We present the latest major release version 6.0 of the quanti-fied Boolean formula (QBF) solver DepQBF, which is based on QCDCL. QCDCL is an extension of the conflict-driven clause learning (CDCL) paradigm implemented in state of the art propositional satisfiability (SAT) solvers. The Q-resolution calculus (QRES) is a QBF proof system which underlies QCDCL. QCDCL solvers can produce QRES proofs of QBFs in prenex conjunctive normal form (PCNF) as a byproduct of the solving process. In contrast to traditional QCDCL based on QRES, DepQBF 6.0 implements a variant of QCDCL which is based on a generalization of QRES. This generalization is due to a set of additional axioms and leaves the original Q-resolution rules unchanged. The generalization of QRES enables QCDCL to potentially produce exponentially shorter proofs than the traditional variant. We present an overview of the features imple-mented in DepQBF and report on experimental results which demonstrate the effectiveness of generalized QRES in QCDCL.},
address = {Gotenburg},
author = {Lonsing, Florian and Egly, Uwe},
booktitle = {International Conference on Automated Deduction},
file = {:Users/jonaprieto/Mendeley/Lonsing, Egly - 2017 - DepQBF 6.0 A Search-Based QBF Solver Beyond Traditional QCDCL.pdf:pdf},
publisher = {Springer},
title = {{DepQBF 6.0: A Search-Based QBF Solver Beyond Traditional QCDCL}},
url = {https://arxiv.org/pdf/1702.08256.pdf},
year = {2017}
}
@phdthesis{Klieber2014,
author = {Klieber, William},
file = {:Users/jonaprieto/Mendeley/Klieber - 2014 - Formal Verification Using Quantified Boolean Formulas (QBF).pdf:pdf},
keywords = {QBF Solvers},
mendeley-tags = {QBF Solvers},
school = {Carnegie Mellon University},
title = {{Formal Verification Using Quantified Boolean Formulas (QBF)}},
url = {http://www.cs.cmu.edu/~wklieber/thesis.pdf},
year = {2014}
}
@incollection{Een2004,
author = {E{\'{e}}n, Niklas and S{\"{o}}rensson, Niklas},
doi = {10.1007/978-3-540-24605-3_37},
file = {:Users/jonaprieto/Mendeley/E{\'{e}}n, S{\"{o}}rensson - 2004 - An Extensible SAT-solver.pdf:pdf},
pages = {502--518},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Extensible SAT-solver}},
url = {http://link.springer.com/10.1007/978-3-540-24605-3_37},
year = {2004}
}
@inproceedings{Moskewicz2001,
abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search - especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
author = {Moskewicz, Matthew W and Madigan, Conor F and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
booktitle = {Proceedings of the 38th Design Automation Conference (DAC 2001)},
doi = {http://doi.acm.org/10.1145/378239.379017},
file = {:Users/jonaprieto/Mendeley/Moskewicz et al. - 2001 - Chaff engineering an efficient SAT solver.pdf:pdf},
isbn = {1-58113-297-2},
issn = {0738-100X},
keywords = {Branching Heuristics,CDCL,Chaff,Lazy Data-Structure,SAT,Solver,VSIDS,Watched literals},
pages = {530--535},
title = {{Chaff: engineering an efficient SAT solver}},
year = {2001}
}
@misc{Schmitt2001,
author = {Schmitt, Stephan and Lorigo, Lori and Kreitz, Christoph and Nogin, Aleksey},
doi = {10.1007/3-540-45744-5_34},
file = {:Users/jonaprieto/Mendeley/Schmitt et al. - 2001 - JProver Integrating Connection-Based Theorem Proving into Interactive Proof Assistants.pdf:pdf;:Users/jonaprieto/Mendeley/Schmitt et al. - 2001 - JProver Integrating Connection-Based Theorem Proving into Interactive Proof Assistants(2).pdf:pdf},
pages = {421--426},
publisher = {Springer, Berlin, Heidelberg},
title = {{JProver: Integrating Connection-Based Theorem Proving into Interactive Proof Assistants}},
url = {http://link.springer.com/10.1007/3-540-45744-5_34},
year = {2001}
}
@incollection{Otten2008,
address = {Berlin, Heidelberg},
author = {Otten, Jens},
booktitle = {Automated Reasoning},
doi = {10.1007/978-3-540-71070-7_23},
file = {:Users/jonaprieto/Mendeley/Otten - 2008 - leanCoP 2.0 and ileanCoP 1.2 High Performance Lean Theorem Proving in Classical and Intuitionistic Logic (System Descript.pdf:pdf},
pages = {283--291},
publisher = {Springer Berlin Heidelberg},
title = {{leanCoP 2.0 and ileanCoP 1.2: High Performance Lean Theorem Proving in Classical and Intuitionistic Logic (System Descriptions)}},
url = {http://link.springer.com/10.1007/978-3-540-71070-7_23},
year = {2008}
}
@incollection{Tammet1996,
author = {Tammet, Tanel},
doi = {10.1007/3-540-61511-3_65},
file = {:Users/jonaprieto/Mendeley/Tammet - 1996 - A Resolution Theorem Prover for Intuitionistic Logic.pdf:pdf},
pages = {2--16},
publisher = {Springer, Berlin, Heidelberg},
title = {{A Resolution Theorem Prover for Intuitionistic Logic}},
url = {http://link.springer.com/10.1007/3-540-61511-3_65},
year = {1996}
}
@techreport{Gomez-Londono2015,
author = {G{\'{o}}mez-Londono, A},
file = {:Users/jonaprieto/Mendeley/G{\'{o}}mez-Londono - 2015 - Proof Reconstruction Parsing Proofs.pdf:pdf},
institution = {EAFIT University},
title = {{Proof Reconstruction: Parsing Proofs}},
url = {http://repository.eafit.edu.co/handle/10784/5484},
year = {2015}
}
@article{Ekici2017,
abstract = {This paper describes SMTCoq, a plug-in for the integration of external solvers into the Coq proof assistant. Based on a checker for generic first-order proof certificates fully implemented and proved correct in Coq, SMTCoq offers facilities to check answers from external SAT and SMT solvers and to increase Coq's automation using such solvers, all in a safeway. The current version supports proof certificates produced by the SAT solver ZChaff, for propositional logic, and the SMT solvers veriT and CVC4, for the quantifier-free fragment of the combined theory of fixed-size bit vectors, functional arrays with extensionality, linear integer arithmetic, and uninterpreted function symbols.},
author = {Ekici, B and Mebsout, A and Tinelli, C and Keller, C and Katz, G},
file = {:Users/jonaprieto/Mendeley/Ekici et al. - 2017 - SMTCoq A plug-in for integrating SMT solvers into Coq.pdf:pdf;:Users/jonaprieto/Mendeley/Ekici et al. - 2017 - SMTCoq A plug-in for integrating SMT solvers into Coq(2).pdf:pdf},
journal = {stanford.edu},
title = {{SMTCoq: A plug-in for integrating SMT solvers into Coq}},
url = {http://web.stanford.edu/~guyk/pub/CAV2017_C.pdf},
year = {2017}
}
@phdthesis{Keller2013,
author = {Keller, Chantal},
file = {:Users/jonaprieto/Mendeley/Keller - 2013 - A Matter of Trust Skeptical Communication Between Coq and External Provers.pdf:pdf},
title = {{A Matter of Trust: Skeptical Communication Between Coq and External Provers}},
url = {https://hal.archives-ouvertes.fr/pastel-00838322/},
year = {2013}
}
@inproceedings{Burel,
abstract = {The $\lambda$$\Pi$-calculus modulo is a proof language that has been proposed as a proof standard for (re-)checking and interoperability. Resolution and superposition are proof-search methods that are used in state-of-the-art first-order automated theorem provers. We provide a shallow embedding of resolution and superposition proofs in the $\lambda$$\Pi$-calculus modulo, thus offering a way to check these proofs in a trusted setting, and to combine them with other proofs. We implement this embedding as a backend of the prover iProver Modulo.},
author = {Burel, Guillaume},
booktitle = {Third International Workshop on Proof Exchange for Theorem Proving,},
file = {:Users/jonaprieto/Mendeley/Burel - 2013 - A Shallow Embedding of Resolution and Superposition Proofs into the $\lambda$ $\Pi$ -Calculus.pdf:pdf},
keywords = {Interoperability,automatic theorem provers,proof checking,rewriting},
mendeley-tags = {Interoperability,automatic theorem provers,proof checking,rewriting},
pages = {1--15},
title = {{A Shallow Embedding of Resolution and Superposition Proofs into the $\lambda$ $\Pi$ -Calculus}},
year = {2013}
}
@article{Mossakowski2005,
author = {Mossakowski, T. and Goguen, J. and Diaconescu, R. and Tarlecki, A.},
file = {:Users/jonaprieto/Mendeley/Mossakowski et al. - 2005 - What is a Logic.pdf:pdf},
journal = {Logica universalis},
keywords = {a version of abstract,abstract,abstract model,categorical logic,category theory,in computer science studies,institution theory,model theory that emerged,of software specifica-,the theory of institutions,theory,this paper builds on,universal logic},
pages = {113--133},
title = {{What is a Logic?}},
url = {http://cseweb.ucsd.edu/~goguen/pps/nel05.pdf},
year = {2005}
}
@book{Curry1963,
author = {Curry, Haskell B},
booktitle = {Journal of the Franklin Institute},
doi = {10.1016/0016-0032(63)90672-0},
file = {:Users/jonaprieto/Mendeley/Curry - 1963 - Foundations of mathematical logic.pdf:pdf},
isbn = {0486634620},
issn = {00160032},
number = {5},
pages = {438},
pmid = {21192365},
title = {{Foundations of mathematical logic}},
volume = {275},
year = {1963}
}
@misc{Pitts1991,
author = {Pitts, Andrew},
file = {:Users/jonaprieto/Mendeley/Pitts - 1991 - Notes on Categorical Logic.pdf:pdf},
title = {{Notes on Categorical Logic}},
year = {1991}
}
@article{Zalta1995,
author = {Zalta, Edward N and Zalta, Edward N},
file = {:Users/jonaprieto/Mendeley/Zalta, Zalta - 1995 - Basic Concepts in Modal Logic.pdf:pdf},
journal = {Society},
pages = {1--91},
title = {{Basic Concepts in Modal Logic}},
year = {1995}
}
@article{Blackburn2005,
author = {Blackburn, Patrick and Benthem, Johan Van},
file = {:Users/jonaprieto/Mendeley/Blackburn, Benthem - 2005 - Modal Logic A Semantic Perspective.pdf:pdf},
pages = {1--50},
title = {{Modal Logic: A Semantic Perspective}},
url = {papers2://publication/uuid/CC567B68-4635-4E4C-A795-06E2E0B6C800},
year = {2005}
}
@article{VanBenthem2010,
abstract = {We show how belief revision can be treated systematically in the format of dynamic- epistemic logic, when operators of conditional belief are added. The core engine consists of de{\{}{\}}nable update rules for changing plausibility relations between worlds, which have been proposed independently in the dynamic-epistemic literature on preference change. Our analysis yields two new types of modal result. First, we obtain complete logics for concrete mechanisms of belief revision, based on compositional reduction axioms. Next, we show how various ab- stract postulates for belief revision can be analyzed by standard modal frame correspondences for model-changing operations.},
author = {van Benthem, Johan},
file = {:Users/jonaprieto/Mendeley/van Benthem - 2010 - Modal Logic for Open Minds.pdf:pdf},
isbn = {0521527147},
journal = {Chart},
number = {199},
pages = {1--390},
pmid = {15957556},
title = {{Modal Logic for Open Minds}},
url = {http://fenrong.net/teaching/mljvb.pdf},
year = {2010}
}
@book{VanBenthem1990a,
abstract = {This volume presents a panorama of the applications of logical tools and methods in the formal analysis of natural language. since a number of few developments in philosophical logic were originally stimulated by concern arising in the semantic analysis of natural language discourse, the chapters in this volume provide some criteria of evaluation of the applications of work in philosophical logic. in revealing both the adequacies and inadequacies of logical investigations in the semantic structures of natural discourse, these chapters also point the way to future developments in philosophical logic in general and thus close again the circle of inquiry relating logic and language.},
author = {van Benthem, Johan},
booktitle = {Language},
doi = {10.2307/414900},
file = {:Users/jonaprieto/Mendeley/van Benthem - 1990 - Handbook of Philosophical Logic, Vol. IV Topics in the Philosophy of Language.pdf:pdf},
isbn = {0198537816},
issn = {00326585},
number = {2},
pages = {555},
title = {{Handbook of Philosophical Logic, Vol. IV: Topics in the Philosophy of Language}},
volume = {66},
year = {1990}
}
@article{Stump2013,
abstract = {Producing and checking proofs from {\{}SMT{\}} solvers is currently the most feasible method for achieving high confidence in the correctness of solver results. The diversity of solvers and relative complexity of {\{}SMT{\}} over, say, {\{}SAT{\}} means that flexibility, as well as performance, is a critical characteristic of a proof-checking solution for {\{}SMT{\}}. This paper describes such a solution, based on a Logical Framework with Side Conditions ({\{}LFSC{\}}). We describe the framework and show how it can be applied for flexible proof production and checking for two different {\{}SMT{\}} solvers, clsat and cvc3. We also report empirical results showing good performance relative to solver execution time.},
author = {Stump, Aaron and Oe, Duckki and Reynolds, Andrew and Hadarean, Liana and Tinelli, Cesare},
doi = {10.1007/s10703-012-0163-3},
file = {:Users/jonaprieto/Mendeley/Stump et al. - 2013 - SMT proof checking using a logical framework.pdf:pdf},
issn = {09259856},
journal = {Formal Methods in System Design},
keywords = {Edinburgh logical framework,LFSC,Proof checking,Satisfiability modulo theories},
number = {1},
pages = {91--118},
title = {{SMT proof checking using a logical framework}},
volume = {42},
year = {2013}
}
@phdthesis{Fleury2015,
abstract = {Various methods have been developed for solving SAT problems, notably resolution, the Davis-Putnam-Logemann-Loveland-Procedure procedure (DPLL) and an extension of it, the conflict-driven clause learning (CDCL). We have formalised these three algorithms in a proof assistant Isabelle/HOL, based on a chapter of ChristophWeidenbach's upcoming book Automed Reasoning – The Art of Generic Problem Solving. The three calculi are presented uniformly as transition systems. We have formally proved that each calculus is a decision procedure for satisfiability of propositional logic. One outcome of the formalization is a verified SAT solver based on DPLL and implemented in a functional programming language.},
author = {Fleury, Mathias},
file = {:Users/jonaprieto/Mendeley/Fleury - 2015 - Formalisation of Ground Inference Systems in a Proof Assistant.pdf:pdf},
title = {{Formalisation of Ground Inference Systems in a Proof Assistant}},
year = {2015}
}
@book{Chlipala2017,
author = {Chlipala, Adam},
booktitle = {MIT Press},
file = {:Users/jonaprieto/Mendeley/Chlipala - 2017 - Certified Programming with Dependent Types.pdf:pdf},
isbn = {9780262317863},
title = {{Certified Programming with Dependent Types}},
url = {http://adam.chlipala.net/cpdt/cpdt.pdf},
year = {2017}
}
@techreport{Fleury2014,
abstract = {Sledgehammer is a powerful interface from Isabelle to automated provers, to discharge subgoals that appear during the interactive proofs. It chooses facts related to this goal and asks some automatic provers to find a proof. The proof can be either reconstructed or just used to extract the relevant lemmas: in both cases the proof is not trusted. We extend the support by adding one first-order prover (Zipperposition), the reconstruction for two higher-order ATPs (Leo-II and Satallax) and an SMT solver veriT. The support of higher-order prover should especially improve Sledgehammer's performance for higher-order goals. Acknowledgement I would like to thank Jasmin Blanchette for the internship about a very interesting subject, and the members of the logic and verification chair for the welcome. Then Simon Cruanes and Pascal Fontaine (developer of Zipperposition and veriT) were very helpful and provided many explanations concerning their provers and bug fixes.},
author = {Fleury, Mathias and Blanchette, Jasmin},
file = {:Users/jonaprieto/Mendeley/Fleury, Blanchette - 2014 - Translation of Proofs Provided by External Provers More Automatic Prover Support for Isabelle Two Higher-Ord.pdf:pdf},
institution = {Techniche Universit{\"{a}}t M{\"{u}}nchen},
keywords = {Isabelle,Leo-II,Satallax,Sledgehammer,TSTP proof,Zipperposition,higher-order proofs,proof reconstruc-tion,veriT},
pages = {2--0},
title = {{Translation of Proofs Provided by External Provers More Automatic Prover Support for Isabelle: Two Higher-Order Provers and a SMT Solver}},
url = {http://perso.eleves.ens-rennes.fr/~mfleur01/documents/Fleury_internship2014.pdf},
year = {2014}
}
@article{Blanchette2016,
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Fleury, Mathias and Smolka, Steffen Juilf and Steckermeier, Albert},
doi = {10.1007/s10817-015-9335-3},
file = {:Users/jonaprieto/Mendeley/Blanchette et al. - 2016 - Semi-intelligible Isar Proofs from Machine-Generated Proofs.pdf:pdf},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Automatic theorem provers,Natural deduction,Proof assistants},
number = {2},
pages = {155--200},
publisher = {Journal of Automated Reasoning},
title = {{Semi-intelligible Isar Proofs from Machine-Generated Proofs}},
url = {http://dx.doi.org/10.1007/s10817-015-9335-3},
volume = {56},
year = {2016}
}
@article{VanDerWalt2013,
abstract = {This paper explores the recent addition to Agda enablin greflection,in the style of Lisp and Template Haskell. It gives a brief introduction to using reflection, and details the complexities encountered when automating certain proofs with proof by reflection. It presents a library that can be used for automatically quoting a class of concrete Agda terms to a non-dependent, user-defined inductive data type, alleviating some of the burden a programmer faces when using reflection in a practical setting.},
author = {{Van Der Walt}, Paul and Swierstra, Wouter},
doi = {10.1007/978-3-642-41582-1_10},
file = {:Users/jonaprieto/Mendeley/Van Der Walt, Swierstra - 2013 - Engineering proof by reflection in Agda.pdf:pdf},
isbn = {9783642415814},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Agda,Dependently-typed programming,Metaprogramming,Proof by reflection,Reflection},
pages = {157--173},
title = {{Engineering proof by reflection in Agda}},
volume = {8241 LNCS},
year = {2013}
}
@phdthesis{Isaza2014,
author = {Isaza, Juan Pedro Villa},
booktitle = {Eafit.Edu.Co},
file = {:Users/jonaprieto/Mendeley/Isaza - 2014 - Category Theory Applied to Functional Programming.pdf:pdf},
keywords = {Category Theory,Haskell},
mendeley-tags = {Category Theory,Haskell},
school = {Universidad EAFIT},
title = {{Category Theory Applied to Functional Programming}},
url = {http://www1.eafit.edu.co/asicard/pubs/cain-screen.pdf},
year = {2014}
}
@article{Norell2009,
abstract = {Dependently typed languages have for a long time been used to describe proofs about programs. Traditionally, dependent types are used mostly for stating and proving the properties of the programs and not in defining the programs themselves. An impressive example is the certified compiler by Leroy (2006) implemented and proved correct in Coq (Bertot and Cast{\'{e}}ran 2004). Recently there has been an increased interest in dependently typed programming, where the aim is to write programs that use the dependent type system to a much higher degree. In this way a lot of the properties that were previously proved separately can be integrated in the type of the program, in many cases adding little or no complexity to the definition of the program. New languages, such as Epigram (McBride and McKinna 2004), are being designed, and existing languages are being extended with new features to accomodate these ideas, for instance the work on dependently typed programming in Coq by Sozeau (2007). This talk gives an overview of the Agda programming language (Norell 2007), whose main focus is on dependently typed programming. Agda provides a rich set of inductive types with a powerful mechanism for pattern matching, allowing dependently typed programs to be written with minimal fuss. To read about programming in Agda, see the lecture notes from the Advanced Functional Programming summer school (Norell 2008) and the work by Oury and Swierstra (2008). In the talk a number of examples of interesting dependently typed programs chosen from the domain of programming language implementation are presented as they are implemented in Agda.},
author = {Norell, Ulf},
doi = {10.1007/978-3-642-04652-0_5},
file = {:Users/jonaprieto/Mendeley/Norell - 2009 - Dependently typed programming in agda.pdf:pdf},
isbn = {3642046517},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Agda 2},
pages = {230--266},
title = {{Dependently typed programming in agda}},
volume = {5832 LNCS},
year = {2009}
}
@inproceedings{Farber2015,
abstract = {Metis is an automated theorem prover based on ordered paramodulation. It is widely employed in the interactive theorem provers Isabelle/HOL and HOL4 to automate proofs as well as reconstruct proofs found by automated provers. For both these purposes, the tableaux-based MESON tactic is frequently used in HOL Light. However, paramodulation-based provers such as Metis perform better on many problems involving equality. We created a Metis-based tactic in HOL Light which translates HOL problems to Metis, runs an OCaml version of Metis, and reconstructs proofs in Metis' paramodulation calculus as HOL proofs. We evaluate the performance of Metis as proof reconstruction method in HOL Light. 1},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
booktitle = {GCAI 2015. Global Conference on Artificial Intelligence Metis-based},
file = {:Users/jonaprieto/Mendeley/F{\"{a}}rber, Kaliszyk - 2015 - Metis-based Paramodulation Tactic for HOL Light.pdf:pdf},
pages = {127--136},
title = {{Metis-based Paramodulation Tactic for HOL Light}},
volume = {36},
year = {2015}
}
@book{TobiasNipkow2016,
author = {Nipkow, Tobias and Klein, Gerwin},
doi = {10.1007/978-3-319-10542-0},
file = {:Users/jonaprieto/Mendeley/Nipkow, Klein - 2016 - Concrete Semantics.pdf:pdf},
isbn = {9783319105413},
pages = {1--2},
title = {{Concrete Semantics}},
year = {2016}
}
@article{Wadler1993,
abstract = {This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of `Propositions as Types'. The presentation of linear logic is simplified by basing it on the Logic of Unity.},
author = {Wadler, Philip},
doi = {10.1007/3-540-57182-5_12},
file = {:Users/jonaprieto/Mendeley/Wadler - 1993 - A taste of linear logic.pdf:pdf},
isbn = {3-540-57182-5},
journal = {Lecture Notes in Computer Science},
number = {September},
pages = {185--210},
title = {{A taste of linear logic}},
url = {http://www.springerlink.com/index/p555h611321h7240.pdf%5Cnhttp://link.springer.com/10.1007/3-540-57182-5_12},
volume = {711},
year = {1993}
}
@book{Jackson2006,
abstract = {{\{}In Software Abstractions Daniel Jackson introduces a new approach to software design that draws on traditional formal methods but exploits automated tools to find flaws as early as possible. This approach--which Jackson calls "lightweight formal methods" or "agile modeling"--takes from formal specification the idea of a precise and expressive notation based on a tiny core of simple and robust concepts but replaces conventional analysis based on theorem proving with a fully automated analysis that gives designers immediate feedback. Jackson has developed Alloy, a language that captures the essence of software abstractions simply and succinctly, using a minimal toolkit of mathematical notions. The designer can use automated analysis not only to correct errors but also to make models that are more precise and elegant. This approach, Jackson says, can rescue designers from "the tarpit of implementation technologies" and return them to thinking deeply about underlying concepts. Software Abstractions introduces the key elements of the approach: a logic, which provides the building blocks of the language; a language, which adds a small amount of syntax to the logic for structuring descriptions; and an analysis, a form of constraint solving that offers both simulation (generating sample states and executions) and checking (finding counterexamples to claimed properties). The book uses Alloy as a vehicle because of its simplicity and tool support, but the book's lessons are mostly language-independent, and could also be applied in the context of other modeling languages.{\}}},
author = {Jackson, Daniel},
booktitle = {MIT Press},
file = {:Users/jonaprieto/Mendeley/Jackson - 2006 - Software Abstractions.pdf:pdf},
number = {02},
pages = {253},
title = {{Software Abstractions}},
volume = {19},
year = {2006}
}
@book{Baier2008,
abstract = {Our growing dependence on increasingly complex computer and software systems necessitates the development of formalisms, techniques, and tools for assessing functional properties of these systems. One such technique that has emerged in the last twenty years is model checking, which systematically (and automatically) checks whether a model of a given system satisfies a desired property such as deadlock freedom, invariants, or request-response properties. This automated technique for verification and debugging has developed into a mature and widely used approach with many applications. Principles of Model Checking offers a comprehensive introduction to model checking that is not only a text suitable for classroom use but also a valuable reference for researchers and practitioners in the field. The book begins with the basic principles for modeling concurrent and communicating systems, introduces different classes of properties (including safety and liveness), presents the notion of fairness, and provides automata- based algorithms for these properties. It introduces the temporal logics LTL and CTL, compares them, and covers algorithms for verifying these logics, discussing real-time systems as well as systems subject to random phenomena. Separate chapters treat such efficiency-improving techniques as abstraction and symbolic manipulation. The book includes an extensive set of examples (most of which run through several chapters) and a complete set of basic results accompanied by detailed proofs. Each chapter concludes with a summary, bibliographic notes, and an extensive list of exercises of both practical and theoretical nature.},
author = {Baier, Christel and Katoen, Joost-Pieter},
booktitle = {MIT Press},
doi = {10.1093/comjnl/bxp025},
file = {:Users/jonaprieto/Mendeley/Baier, Katoen - 2008 - Principles Of Model Checking.pdf:pdf;:Users/jonaprieto/Mendeley/Baier, Katoen - 2008 - Principles Of Model Checking(2).pdf:pdf},
isbn = {9780262026499},
issn = {00155713},
pages = {I--XVII, 1--975},
pmid = {11275744},
title = {{Principles Of Model Checking}},
url = {http://mitpress.mit.edu/books/principles-model-checking},
volume = {950},
year = {2008}
}
@inproceedings{Bohme2011,
abstract = {Automatic provers that can produce proof certificates do not need to be trusted. The certificate can be checked by an independent tool, for example an LCF-style proof assistant such as IsabelleHOL or HOL. Currently, the design of proof formats is mostly dictated by internal constraints of automatic provers and less guided by applications such as checking of certificates. In the worst case, checking can be as involved as the actual proof search simply because important information is missing in the proof certificate. To address this and other issues, we describe design choices for proof formats that we consider both feasible for implementors of automatic provers as well as effective to simplify checking of certificates.},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {First International Workshop on Proof eXchange for Theorem Proving - PxTP 2011},
file = {:Users/jonaprieto/Mendeley/B{\"{o}}hme, Weber - 2011 - Designing Proof Formats A User's Perspective - Experience Report.pdf:pdf;:Users/jonaprieto/Mendeley/B{\"{o}}hme, Weber - 2011 - Designing Proof Formats A User's Perspective - Experience Report(2).pdf:pdf},
title = {{Designing Proof Formats: A User's Perspective - Experience Report}},
url = {http://hal.inria.fr/hal-00677244},
year = {2011}
}
@article{Altenkirch2015,
abstract = {A computer formalisation of the completeness of the boolean model of classical propositional logic is presented. The work follows Huth and Ryan's proof [9]. The proof is constructed for a classical logic system in natural deduction style with all logical connectives. The formalisation is constructive and uses the interactive theorem prover Agda which is an implementation of intensional Martin-L{\"{o}}f type theory [11]. Functions have to be defined in a structurally recursive way to pass the termination checker of Agda. The basic definitions of the formal system must be carefully chosen in order to provide a convenient environment to build correct proofs and meanwhile prevent from getting warnings from the type checker. The formalisation is written in an accessible way so that it can be used for educational purposes. The full source code is available online1. Keywords:},
author = {Cai, Leran and Kaposi, Ambrus and Altenkirch, Thorsten},
file = {:Users/jonaprieto/Mendeley/Cai, Kaposi, Altenkirch - 2015 - Formalising the Completeness Theorem of Classical Propositional Logic in Agda.pdf:pdf},
title = {{Formalising the Completeness Theorem of Classical Propositional Logic in Agda}},
url = {https://akaposi.github.io/proplogic.pdf},
year = {2015}
}
@inproceedings{DeMoura2008,
abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
author = {{De Moura}, Leonardo and Bj{\o}rner, Nikolaj},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-78800-3_24},
isbn = {3540787992},
issn = {03029743},
pages = {337--340},
title = {{Z3: An efficient SMT Solver}},
volume = {4963 LNCS},
year = {2008}
}
@article{kanso2016light,
author = {Kanso, Karim and Setzer, Anton},
doi = {10.1017/S0960129514000140},
file = {:Users/jonaprieto/Mendeley/Kanso, Setzer - 2016 - A Light-weight Integration Of Automated And Interactive Theorem Proving.pdf:pdf},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
number = {1},
pages = {129--153},
publisher = {Cambridge University Press},
title = {{A Light-weight Integration Of Automated And Interactive Theorem Proving}},
url = {http://cs.swansea.ac.uk/~cskarim/agda/},
volume = {26},
year = {2016}
}
@inproceedings{Weber2006,
abstract = {This paper describes the integration of a leading SAT solver with Isabelle/HOL, a popular interactive theorem prover. The SAT solver generates resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem prover. The presented approach significantly improves Isabelle's performance on propositional problems, and furthermore exhibits counterexamples for unprovable conjectures. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Weber, Tjark},
booktitle = {Electronic Notes in Theoretical Computer Science},
doi = {10.1016/j.entcs.2005.12.007},
file = {:Users/jonaprieto/Mendeley/Weber - 2006 - Integrating a SAT solver with an LCF-style theorem prover.pdf:pdf},
issn = {15710661},
keywords = {LCF-style theorem prover,Proof checking,Propositional resolution,SAT solver},
number = {2 SPEC. ISS.},
pages = {67--78},
title = {{Integrating a SAT solver with an LCF-style theorem prover}},
volume = {144},
year = {2006}
}
@article{Fontaine2006,
abstract = {Formal system development needs expressive specification languages, but also calls for highly automated tools. These two goals are not easy to reconcile, especially if one also aims at high assurances for correctness. In this paper, we describe a combination of {\{}Isabelle/HOL{\}} with a proof-producing {\{}SMT{\}} (Satisfiability Modulo Theories) solver that contains a {\{}SAT{\}} engine and a decision procedure for quantifier-free first-order logic with equality. As a result, a user benefits from the expressiveness of {\{}Isabelle/HOL{\}} when modeling a system, but obtains much better automation for those fragments of the proofs that fall within the scope of the (automatic) {\{}SMT{\}} solver. Soundness is not compromised because all proofs are submitted to the trusted kernel of Isabelle for certification. This architecture is straightforward to extend for other interactive proof assistants and proof-producing reasoners.},
author = {Fontaine, Pascal and Marion, Jean Yves and Merz, Stephan and Nieto, Leonor Prensa and Tiu, Alwen},
doi = {10.1007/11691372_11},
file = {:Users/jonaprieto/Mendeley/Fontaine et al. - 2006 - Expressiveness automation soundness Towards combining SMT solvers and interactive proof assistants.pdf:pdf},
isbn = {3540330569},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {167--181},
title = {{Expressiveness + automation + soundness: Towards combining SMT solvers and interactive proof assistants}},
volume = {3920 LNCS},
year = {2006}
}
@incollection{Riazanov1999,
abstract = {Vampire is a resolution-based theorem prover for rst-order classical logic. The current version implements ordered binary resolution with the set-of-support strategy and ordered hyperresolution. The competition version will have equality rules.},
address = {Berlin, Heidelberg},
author = {Riazanov, Alexandre and Voronkov, Andrei},
booktitle = {Automated Deduction --- CADE-16: 16th International Conference on Automated Deduction Trento, Italy, July 7--10, 1999 Proceedings},
doi = {10.1007/3-540-48660-7_26},
isbn = {978-3-540-48660-2},
pages = {292--296},
publisher = {Springer Berlin Heidelberg},
title = {{Vampire}},
url = {https://doi.org/10.1007/3-540-48660-7_26},
year = {1999}
}
@article{Farber2016,
abstract = {Copyright ? by the paper's authors.Proof assistants based on higher-order logic frequently use first-order automated theorem provers as proof search mechanisms. The reconstruction of the proofs generated by common tools, such as MESON and Metis, typically involves the use of the axiom of choice to simulate the Skolemisation steps. In this paper we present a method to reconstruct the proofs without introducing Skolem functions. This enables us to integrate tactics that use first-order automated theorem provers in logics that feature neither the axiom of choice nor the definite description operator.},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
file = {:Users/jonaprieto/Mendeley/F{\"{a}}rber, Kaliszyk - 2016 - No choice Reconstruction of first-order ATP proofs without skolem functions.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {Paar},
pages = {24--31},
title = {{No choice: Reconstruction of first-order ATP proofs without skolem functions}},
volume = {1635},
year = {2016}
}
@article{bezem2002automated,
author = {Bezem, Marc and Hendriks, Dimitri and {De Nivelle}, Hans},
journal = {Journal of Automated Reasoning},
number = {3},
pages = {253--275},
publisher = {Springer},
title = {{Automated Proof Construction in Type Theory Using Resolution}},
volume = {29},
year = {2002}
}
@article{hurd2003first,
abstract = {In this paper we evaluate the effectiveness of first-order proof procedures when used as tactics for proving subgoals in a higher-order logic interactive theorem prover. We first motivate why such first-order proof tactics are useful, and then describe the core integrating technology: an `LCF-style' logical kernel for clausal first-order logic. This allows the choice of different logical mappings between higher-order logic and first-order logic to be used depending on the subgoal, and also enables several different first-order proof procedures to cooperate on constructing the proof. This work was carried out using the HOL4 theorem prover; we comment on the ease of transferring the technology to other higher-order logic theorem provers},
author = {Hurd, Joe},
file = {:Users/jonaprieto/Mendeley/Hurd - 2003 - First-order Proof Tactics In Higher-order Logic Theorem Provers.pdf:pdf},
journal = {Design and Application of Strategies/Tactics in Higher Order Logics, number NASA/CP-2003-212448 in NASA Technical Reports},
pages = {56--68},
title = {{First-order Proof Tactics In Higher-order Logic Theorem Provers}},
url = {http://www.gilith.com/research/papers},
year = {2003}
}
@inproceedings{paulson2010three,
author = {Paulson, Lawrence C and Blanchette, Jasmin},
booktitle = {PAAR@ IJCAR},
file = {:Users/jonaprieto/Mendeley/Paulson, Blanchette - 2010 - Three Years Of Experience with Sledgehammer, A Practical Link Between Automatic And Interactive Theorem Pro.pdf:pdf},
pages = {1--10},
title = {{Three Years Of Experience with Sledgehammer, A Practical Link Between Automatic And Interactive Theorem Provers.}},
year = {2010}
}
@incollection{Barrett2011,
abstract = {CVC4 is the latest version of the Cooperating Validity Checker. A joint project of NYU and U Iowa, CVC4 aims to support the useful feature set of CVC3 and SMT-LIBv2 while optimizing the design of the core system architecture and decision procedures to take advantage of recent engineering and algorithmic advances. CVC4 represents a completely new code base; it is a from-scratch rewrite of CVC3, and many subsystems have been completely redesigned. Additional decision procedures for CVC4 are currently under development, but for what it currently achieves, it is a lighter-weight and higher-performing tool than CVC3. We describe the system architecture, subsystems of note, and discuss some applications and continuing work.},
address = {Berlin, Heidelberg},
author = {Barrett, Clark and Conway, Christopher L and Deters, Morgan and Hadarean, Liana and Jovanovi{\'{c}}, Dejan and King, Tim and Reynolds, Andrew and Tinelli, Cesare},
booktitle = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings},
doi = {10.1007/978-3-642-22110-1_14},
editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
isbn = {978-3-642-22110-1},
pages = {171--177},
publisher = {Springer Berlin Heidelberg},
title = {{CVC4}},
year = {2011}
}
@inproceedings{paulson2007source,
author = {Paulson, Lawrence C and Susanto, Kong Woei},
booktitle = {TPHOLs},
file = {:Users/jonaprieto/Mendeley/Paulson, Susanto - 2007 - Source-level Proof Reconstruction For Interactive Theorem Proving.pdf:pdf},
organization = {Springer},
pages = {232--245},
title = {{Source-level Proof Reconstruction For Interactive Theorem Proving}},
volume = {4732},
year = {2007}
}
@misc{denivelle2003,
author = {{De Nivelle}, Hans},
title = {{Bliksem 1.10 User Manual}},
url = {http://www.ii.uni.wroc.pl/~nivelle/software/bliksem.}
}
@incollection{armand2011,
abstract = {We present a way to enjoy the power of SAT and SMT provers in Coq without compromising soundness. This requires these provers to return not only a yes/no answer, but also a proof witness that can be independently rechecked. We present such a checker, written and fully certified in Coq. It is conceived in a modular way, in order to tame the proofs' complexity and to be extendable. It can currently check witnesses from the SAT solver ZChaff and from the SMT solver veriT. Experiments highlight the efficiency of this checker. On top of it, new reflexive Coq tactics have been built that can decide a subset of Coq's logic by calling external provers and carefully checking their answers.},
address = {Berlin, Heidelberg},
author = {Armand, Michael and Faure, Germain and Gr{\'{e}}goire, Benjamin and Keller, Chantal and Th{\'{e}}ry, Laurent and Werner, Benjamin},
booktitle = {Certified Programs and Proofs: First International Conference, CPP 2011, Kenting, Taiwan, December 7-9, 2011. Proceedings},
doi = {10.1007/978-3-642-25379-9_12},
isbn = {978-3-642-25379-9},
pages = {135--150},
publisher = {Springer Berlin Heidelberg},
title = {{A Modular Integration of SAT/SMT Solvers to Coq through Proof Witnesses}},
url = {https://doi.org/10.1007/978-3-642-25379-9_12},
year = {2011}
}
@incollection{bouton2009,
abstract = {This article describes the first public version of the satisfiability modulo theory (SMT) solver veriT. It is open-source, proof-producing, and complete for quantifier-free formulas with uninterpreted functions and difference logic on real numbers and integers.},
address = {Berlin, Heidelberg},
author = {Bouton, Thomas and de Oliveira, Diego and D{\'{e}}harbe, David and Fontaine, Pascal},
booktitle = {Automated Deduction -- CADE-22: 22nd International Conference on Automated Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},
doi = {10.1007/978-3-642-02959-2_12},
isbn = {978-3-642-02959-2},
pages = {151--156},
publisher = {Springer Berlin Heidelberg},
title = {{veriT: An Open, Trustable and Efficient SMT-Solver}},
year = {2009}
}
@book{humberstone2011,
author = {Humberstone, L},
publisher = {MIT Press},
title = {{The Connectives}},
year = {2011}
}
@book{paulson1994isabelle,
author = {Paulson, Lawrence C},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle: A Generic Theorem Prover}},
volume = {828},
year = {1994}
}
@article{appel1959,
author = {Appel, K I},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {4},
pages = {306--310},
publisher = {Association for Symbolic Logic},
title = {{Horn Sentences in Identity Theory}},
url = {http://www.jstor.org/stable/2963901},
volume = {24},
year = {1959}
}
@article{meng2006automation,
author = {Meng, Jia and Quigley, Claire and Paulson, Lawrence C},
journal = {Information and computation},
number = {10},
pages = {1575--1596},
publisher = {Elsevier},
title = {{Automation for Interactive Proof: First Prototype}},
volume = {204},
year = {2006}
}
@book{nipkow2002isabelle,
author = {Nipkow, Tobias and Paulson, Lawrence C and Wenzel, Markus},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle/HOL: A Proof Assistant for Higher-order Logic}},
volume = {2283},
year = {2002}
}
@article{sutcliffe2009,
abstract = {This paper describes the First-Order Form (FOF) and Clause Normal Form (CNF) parts of the TPTP problem library, and the associated infrastructure. TPTP v3.5.0 was the last release containing only FOF and CNF problems, and thus serves as the exemplar. This paper summarizes the history and development of the TPTP, describes the structure and contents of the TPTP, and gives an overview of TPTP related projects and tools.},
author = {Sutcliffe, Geoff},
doi = {10.1007/s10817-009-9143-8},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
month = {jul},
number = {4},
pages = {337},
title = {{The TPTP Problem Library and Associated Infrastructure}},
url = {https://doi.org/10.1007/s10817-009-9143-8},
volume = {43},
year = {2009}
}
@incollection{kaliszyk2013,
abstract = {PRocH is a proof reconstruction tool that imports in HOL Light proofs produced by ATPs on the recently developed translation of HOL Light and Flyspeck problems to ATP formats. PRocH combines several reconstruction methods in parallel, but the core improvement over previous methods is obtained by re-playing in the HOL logic the detailed inference steps recorded in the ATP (TPTP) proofs, using several internal HOL Light inference methods. These methods range from fast variable matching and more involved rewriting, to full first-order theorem proving using the MESON tactic. The system is described and its performance is evaluated here on a large set of Flyspeck problems.},
address = {Berlin, Heidelberg},
author = {Kaliszyk, Cezary and Urban, Josef},
booktitle = {Automated Deduction -- CADE-24: 24th International Conference on Automated Deduction, Lake Placid, NY, USA, June 9-14, 2013. Proceedings},
doi = {10.1007/978-3-642-38574-2_18},
editor = {Bonacina, Maria Paola},
file = {:Users/jonaprieto/Mendeley/Kaliszyk, Urban - 2013 - PRocH Proof Reconstruction for HOL Light.pdf:pdf},
isbn = {978-3-642-38574-2},
pages = {267--274},
publisher = {Springer Berlin Heidelberg},
title = {{PRocH: Proof Reconstruction for HOL Light}},
url = {https://doi.org/10.1007/978-3-642-38574-2_18},
year = {2013}
}
@incollection{sultana2015,
abstract = {Implementing proof reconstruction is difficult because it involves symbolic manipulations of formal objects whose representation varies between different systems. It requires significant knowledge of the source and target systems. One cannot simply re-target to another logic. We present a modular proof reconstruction system with separate components, specifying their behaviour and describing how they interact. This system is demonstrated and evaluated through an implementation to reconstruct proofs generated by Leo-II and Satallax in Isabelle HOL, and is shown to work better than the current method of rediscovering proofs using a select set of provers.},
address = {Cham},
author = {Sultana, Nik and Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C},
booktitle = {Frontiers of Combining Systems: 10th International Symposium, FroCoS 2015, Wroclaw, Poland, September 21-24, 2015, Proceedings},
doi = {10.1007/978-3-319-24246-0_16},
editor = {Lutz, Carsten and Ranise, Silvio},
file = {:Users/jonaprieto/Mendeley/Sultana, Benzm{\"{u}}ller, Paulson - 2015 - Proofs and Reconstructions.pdf:pdf},
isbn = {978-3-319-24246-0},
pages = {256--271},
publisher = {Springer International Publishing},
title = {{Proofs and Reconstructions}},
url = {https://doi.org/10.1007/978-3-319-24246-0_16},
year = {2015}
}
@inproceedings{hurlin07practical,
address = {Bremen, Germany},
author = {Hurlin, Cl'ment and Chaieb, Amine and Fontaine, Pascal and Merz, Stephan and Weber, Tjark},
booktitle = {Proceedings of the Isabelle Workshop 2007},
editor = {Dixon, Lucas and Johansson, Moa},
file = {:Users/jonaprieto/Mendeley/Hurlin et al. - 2007 - Practical Proof Reconstruction for First-Order Logic and Set-Theoretical Constructions.pdf:pdf},
pages = {2--13},
title = {{Practical Proof Reconstruction for First-Order Logic and Set-Theoretical Constructions}},
year = {2007}
}
@incollection{foster2011integrating,
abstract = {Agda is a dependently typed functional programming language and a proof assistant in which developing programs and proving their correctness is one activity. We show how this process can be enhanced by integrating external automated theorem provers, provide a prototypical integration of the equational theorem prover Waldmeister, and give examples of how this proof automation works in practice.},
address = {Berlin, Heidelberg},
author = {Foster, Simon and Struth, Georg},
booktitle = {NASA Formal Methods: Third International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20, 2011. Proceedings},
doi = {10.1007/978-3-642-20398-5_10},
file = {:Users/jonaprieto/Mendeley/Foster, Struth - 2011 - Integrating an Automated Theorem Prover into Agda.pdf:pdf},
isbn = {978-3-642-20398-5},
pages = {116--130},
publisher = {Springer Berlin Heidelberg},
title = {{Integrating an Automated Theorem Prover into Agda}},
year = {2011}
}
@article{blanchette2013extending,
abstract = {Sledgehammer is a component of Isabelle/HOL that employs resolution-based first-order automatic theorem provers (ATPs) to discharge goals arising in interactive proofs. It heuristically selects relevant facts and, if an ATP is successful, produces a snippet that replays the proof in Isabelle. We extended Sledgehammer to invoke satisfiability modulo theories (SMT) solvers as well, exploiting its relevance filter and parallel architecture. The ATPs and SMT solvers nicely complement each other, and Isabelle users are now pleasantly surprised by SMT proofs for problems beyond the ATPs' reach.},
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Paulson, Lawrence C},
doi = {10.1007/s10817-013-9278-5},
file = {:Users/jonaprieto/Mendeley/Blanchette, B{\"{o}}hme, Paulson - 2013 - Extending Sledgehammer with SMT Solvers(2).pdf:pdf},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
month = {jun},
number = {1},
pages = {109--128},
title = {{Extending Sledgehammer with SMT Solvers}},
url = {https://doi.org/10.1007/s10817-013-9278-5},
volume = {51},
year = {2013}
}
@incollection{bohme2010,
abstract = {The Satisfiability Modulo Theories (SMT) solver Z3 can generate proofs of unsatisfiability. We present independent reconstruction of these proofs in the theorem provers Isabelle/HOL and HOL4 with particular focus on efficiency. Our highly optimized implementations outperform previous LCF-style proof checkers for SMT, often by orders of magnitude. Detailed performance data shows that LCF-style proof reconstruction can be faster than proof search in Z3.},
address = {Berlin, Heidelberg},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {Interactive Theorem Proving: First International Conference, ITP 2010, Edinburgh, UK, July 11-14, 2010. Proceedings},
doi = {10.1007/978-3-642-14052-5_14},
editor = {Kaufmann, Matt and Paulson, Lawrence C},
file = {:Users/jonaprieto/Mendeley/B{\"{o}}hme, Weber - 2010 - Fast LCF-Style Proof Reconstruction for Z3.pdf:pdf;:Users/jonaprieto/Mendeley/B{\"{o}}hme, Weber - 2010 - Fast LCF-Style Proof Reconstruction for Z3(2).pdf:pdf},
isbn = {978-3-642-14052-5},
pages = {179--194},
publisher = {Springer Berlin Heidelberg},
title = {{Fast LCF-Style Proof Reconstruction for Z3}},
year = {2010}
}
@misc{norrish2007hol,
author = {Norrish, Michael and Slind, Konrad},
title = {{The HOL system description}},
year = {2007}
}
@misc{agdateam,
author = {Team, The Agda Developement},
edition = {Version 8.},
title = {{Agda 2.4.2.3}},
url = {http://wiki.portal.chalmers.se/agda/pmwiki.php},
year = {2015}
}
@article{sutcliffe2004tstp,
author = {Sutcliffe, Geoff and Zimmer, J{\"{u}}rgen and Schulz, Stephan},
journal = {Distributed Constraint Problem Solving and Reasoning in Multi-Agent Systems},
pages = {201--215},
title = {{TSTP data-exchange formats for automated theorem proving tools}},
volume = {112},
year = {2004}
}
@article{Schulz:AICOM-2002,
annote = {StS},
author = {Schulz, Stephan},
journal = {Journal of AI Communications},
number = {2/3},
pages = {111--126},
title = {{E -- A Brainiac Theorem Prover}},
volume = {15},
year = {2002}
}
@misc{coqteam,
author = {Team, The Coq Developement},
edition = {Version 8.},
title = {{The Coq Proof Assistant. Reference Manual}},
url = {https://coq.inria.fr/distrib/current/files/Reference-Manual.pdf},
year = {2015}
}
@inproceedings{Sut07-CSR,
author = {Sutcliffe, G},
booktitle = {Proceedings of the 2nd International Computer Science Symposium in Russia},
editor = {Diekert, V and Volkov, M and Voronkov, A},
number = {4649},
pages = {7--23},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{TPTP, TSTP, CASC, etc.}},
year = {2007}
}
@article{hillenbrand1997,
abstract = {Waldmeister is a high-performance theorem prover for unit equational first-order logic. In the making of Waldmeister, we have applied an engineering approach, identifying the critical points with respect to efficiency in time and space. Our logical three-level system model consists of the basic operations on the lowest level, where we put great stress on efficient data structures and algorithms. For the middle level, where the inference steps are aggregated into an inference machine, flexible adjustment has proven essential during experimental evaluation. The top level holds control strategy and reduction ordering. Although at this level only standard strategies are employed, really large proof tasks have been managed in reasonable time.},
author = {Hillenbrand, Thomas and Buch, Arnim and Vogt, Roland and L{\"{o}}chner, Bernd},
doi = {10.1023/A:1005872405899},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
number = {2},
pages = {265--270},
title = {{WALDMEISTER - High-Performance Equational Deduction}},
url = {https://doi.org/10.1023/A:1005872405899},
volume = {18},
year = {1997}
}
@article{Weber2009,
abstract = {This paper describes the integration of zChaff and MiniSat, currently two leading SAT solvers, with Higher Order Logic (HOL) theorem provers. Both SAT solvers generate resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem provers. The presented approach significantly improves the provers' performance on propositional problems, and exhibits counterexamples for unprovable conjectures. It is also shown that LCF-style theorem provers can serve as viable proof checkers even for large SAT problems. An efficient representation of the propositional problem in the theorem prover turns out to be crucial; several possible solutions are discussed. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Weber, Tjark and Amjad, Hasan},
doi = {10.1016/j.jal.2007.07.003},
file = {:Users/jonaprieto/Mendeley/Weber, Amjad - 2009 - Efficiently checking propositional refutations in HOL theorem provers.pdf:pdf},
issn = {15708683},
journal = {Journal of Applied Logic},
keywords = {Interactive theorem proving,LCF-style proof checking,Propositional resolution},
number = {1},
pages = {26--40},
title = {{Efficiently checking propositional refutations in HOL theorem provers}},
volume = {7},
year = {2009}
}
@misc{Hurd1999,
author = {Hurd, Joe},
doi = {10.1007/3-540-48256-3_21},
file = {:Users/jonaprieto/Mendeley/Hurd - 1999 - Integrating Gandalf and HOL.pdf:pdf},
pages = {311--321},
publisher = {Springer, Berlin, Heidelberg},
title = {{Integrating Gandalf and HOL}},
url = {http://link.springer.com/10.1007/3-540-48256-3_21},
year = {1999}
}
@inproceedings{Weidenbach2009,
abstract = {SPASS is an automated theorem prover for full first-order logic with equality and a number of non-classical logics. This system description provides an overview of our recent developments in SPASS 3.5 including subterm contextual rewriting, improved split backtracking, a significantly faster FLOTTER implementation with additional control flags, completely symmetric implementation of forward and backward redundancy criteria, faster parsing with improved support for big files, faster and extended sort module, and support for include commands in input files. Finally, SPASS 3.5 can now parse files in TPTP syntax, comes with a new converter tptp2dfg and is distributed under a BSD style license.},
author = {Weidenbach, Christoph and Dimova, Dilyana and Fietzke, Arnaud and Kumar, Rohit and Suda, Martin and Wischnewski, Patrick},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02959-2_10},
isbn = {3642029582},
issn = {03029743},
pages = {140--145},
title = {{SPASS version 3.5}},
volume = {5663 LNAI},
year = {2009}
}
